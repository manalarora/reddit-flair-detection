{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataScrape.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5G2ygMEfz8D",
        "colab_type": "text"
      },
      "source": [
        "##Install Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfOCyumlEv9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install praw\n",
        "# !pip install pandas\n",
        "# !pip install nltk\n",
        "# !pip install bs4\n",
        "# !pip install re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16jIGCHJhSz0",
        "colab_type": "text"
      },
      "source": [
        "##Import Libraires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkEVlnkhJc8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF_BMa31htCL",
        "colab_type": "text"
      },
      "source": [
        "##Praw variable declerations\n",
        "PRAW is a Python wrapper for the Reddit API, which enables you to scrape data from subreddits. Here we define the subredit as India as we have to scrape data from that sub-reddit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrRaKVLkJdMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit = praw.Reddit(client_id = \"b5GZswE_-4JHvw\",\n",
        "                     client_secret = \"weXodwTvLzJkPcnmfyP72DTs184\",\n",
        "                     user_agent = \"Reddit Flare Detection\",\n",
        "                     username = \"allergy21\",\n",
        "                     password = \"##rkueCQf7ZGez!\")\n",
        "\n",
        "subreddit = reddit.subreddit('india')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnSF-CDlYGCC",
        "colab_type": "text"
      },
      "source": [
        "##Flair categories\n",
        "These are the flair categories (our labels) which we will classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RnDOc1sJds5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flairs = [\"AskIndia\", \"Non-Political\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"Coronavirus\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge93YFQPYVgT",
        "colab_type": "text"
      },
      "source": [
        "##Scarping data from reddit\n",
        "Here we scrape 100 posts about each flair from the r/india subreddit and save it to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kxyvVHS58tv",
        "colab_type": "code",
        "outputId": "3b067f25-6b68-4105-8685-0e7b25a71760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "topics_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
        "\n",
        "for flair in flairs:\n",
        "  print(flair)\n",
        "  posts = subreddit.search(flair, limit=150)\n",
        "\n",
        "  for post in posts:\n",
        "    topics_dict[\"flair\"].append(str(flair))\n",
        "    topics_dict[\"title\"].append(str(post.title))\n",
        "    topics_dict[\"score\"].append(str(post.score))\n",
        "    topics_dict[\"id\"].append(str(post.id))\n",
        "    topics_dict[\"url\"].append(str(post.url))\n",
        "    topics_dict[\"comms_num\"].append(str(post.num_comments))\n",
        "    topics_dict[\"created\"].append(str(post.created))\n",
        "    topics_dict[\"body\"].append(str(post.selftext))\n",
        "    topics_dict[\"author\"].append(str(post.author))\n",
        "\n",
        "    # add comments\n",
        "    post.comments.replace_more(limit=None)\n",
        "    comment = ''\n",
        "    count = 0\n",
        "    for top_level_comment in post.comments:\n",
        "      comment = comment + ' ' + top_level_comment.body\n",
        "      count+=1     \n",
        "      if(count > 10):\n",
        "        break\n",
        "    topics_dict[\"comments\"].append(comment)\n",
        "\n",
        "info = pd.DataFrame(topics_dict)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AskIndia\n",
            "Non-Political\n",
            "Scheduled\n",
            "Photography\n",
            "Science/Technology\n",
            "Politics\n",
            "Business/Finance\n",
            "Policy/Economy\n",
            "Sports\n",
            "Food\n",
            "Coronavirus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b9Vy1nMY9nG",
        "colab_type": "text"
      },
      "source": [
        "##Data Cleaning Functions\n",
        "We define fuctions for data cleaning/pre-processing to remove unwanted symbols and stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3atNInQ-6oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "replace_symbol = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = replace_by_space.sub(' ', text) # replace certain symbols by space in text\n",
        "    text = replace_symbol.sub('', text) # delete symbols from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove STOPWORDS from text\n",
        "    return text\n",
        "\n",
        "def to_str(text):\n",
        "  return str(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqAcgNc1ZdAB",
        "colab_type": "text"
      },
      "source": [
        "##Clean the data and save it into a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kibhX6KW_3pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "info['title'] = info['title'].apply(to_str)\n",
        "info['body'] = info['body'].apply(to_str)\n",
        "info['comments'] = info['comments'].apply(to_str)\n",
        "\n",
        "info['title'] = info['title'].apply(clean_text)\n",
        "info['body'] = info['body'].apply(clean_text)\n",
        "info['comments'] = info['comments'].apply(clean_text)\n",
        "\n",
        "combined_features = info[\"title\"] + info[\"comments\"] \n",
        "info = info.assign(combined_features = combined_features)\n",
        "\n",
        "#saving the csv file\n",
        "info.to_csv('data5.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}